# ä¿å­˜canvas+åœæ­¢ç»˜ç”»ï¼Ÿï¼Ÿï¼Ÿ
import numpy as np
import os
import cv2
from tracking import HandTrackingModule as htm
import torch
from ml.cnntrain_mnist import ConvNet
from torchvision import transforms
import sys
from PIL import Image

_DIGIT_MODEL = None

def _get_digit_model():
    global _DIGIT_MODEL
    if _DIGIT_MODEL is None:
        from utils import paths
        # å…¼å®¹å†å²ä¿å­˜æ–¹å¼ï¼šæ¨¡å‹å¯èƒ½ä»¥ __main__.ConvNet åç§°è¢«pickle
        try:
            # å°†å½“å‰åŒ…å†…çš„ ConvNet æ˜ å°„åˆ° __main__.ConvNetï¼Œä¾¿äº torch.load å®šä½
            import ml.cnntrain_mnist as cm
            setattr(sys.modules.get('__main__'), 'ConvNet', cm.ConvNet)
        except Exception:
            pass
        _DIGIT_MODEL = torch.load(str(paths.models_dir() / 'MNIST1.pth'), map_location='cpu')
        _DIGIT_MODEL.eval()
    return _DIGIT_MODEL

def recognize_digit_from_image(image_path):
    """ä½¿ç”¨number_rec.pyçš„è¯†åˆ«é€»è¾‘è¯†åˆ«å›¾ç‰‡ä¸­çš„æ•°å­—"""
    try:
        # ä»…é¦–æ¬¡åŠ è½½æ¨¡å‹ï¼Œåç»­å¤ç”¨ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡è¯†åˆ«éƒ½é‡å¤åŠ è½½
        model = _get_digit_model()
        
        # è¯»å–å›¾ç‰‡ (ä½¿ç”¨number_rec.pyä¸­çš„é€»è¾‘)
        img = cv2.imread(image_path, 0)  # ç°åº¦å›¾
        if img is None:
            return None
            
        # æå–ç»˜ç”»åŒºåŸŸ (å’Œnumber_rec.pyä¸€æ ·)
        img = img[0:400, 400:800] 
        
        # é”åŒ–å’Œæ¨¡ç³Š (number_rec.pyçš„å¤„ç†)
        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], np.float32)
        img = cv2.filter2D(img, -1, kernel=kernel)
        img = cv2.blur(img, (15, 1))
        
        # è°ƒæ•´å¤§å°åˆ°20x20 (ä¿®å¤ANTIALIASé”™è¯¯)
        img = cv2.resize(img, (20, 20), interpolation=cv2.INTER_LANCZOS4)
        
        # è®¡ç®—è´¨å¿ƒå¹¶å±…ä¸­ (number_rec.pyçš„é€»è¾‘)
        ret, thresh = cv2.threshold(img, 127, 255, 0)
        M = cv2.moments(thresh)
        if M["m00"] != 0:
            cx = (M["m10"] / M["m00"])
            cy = (M["m01"] / M["m00"])
            M = np.float32([[1,0,(10-cx)],[0,1,10-cy]])
            img = cv2.warpAffine(img, M, (20, 20))
        
        # æ·»åŠ è¾¹æ¡†åˆ°28x28
        img = cv2.copyMakeBorder(img, 4, 4, 4, 4, cv2.BORDER_CONSTANT, value=[0,0,0])
        
        # è½¬æ¢å’Œé¢„æµ‹ (number_rec.pyçš„é€»è¾‘)
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        img_tensor = transform(img).unsqueeze(0)
        output = model(img_tensor)
        label = torch.argmax(output)
        index = label.item()
        
        return index
        
    except Exception as e:
        print(f"è¯†åˆ«é”™è¯¯: {e}")
        return None

def written_number(ui=None):
    brushThickness = 50
    eraserThickness = 15

    drawColor = (255, 255, 255)
    videoSourceIndex = 0
    # Use V4L2 backend for Linux instead of DSHOW (Windows-specific)
    cap = cv2.VideoCapture(videoSourceIndex, cv2.CAP_V4L2)
    if not cap.isOpened():
        # å›é€€åˆ°é»˜è®¤åç«¯ï¼ˆè·¨å¹³å°ï¼‰
        cap = cv2.VideoCapture(videoSourceIndex)
    
    # Check if camera opened successfully
    if not cap.isOpened():
        print(f"é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´ç´¢å¼• {videoSourceIndex}")
        return
    
    cap.set(3, 1280)#é•¿
    cap.set(4, 720)#å®½

    detector = htm.handDetector()
    xp, yp = 0, 0
    imgCanvas = np.zeros((720, 1280, 3), np.uint8)
    
    # è¯†åˆ«ç›¸å…³å˜é‡
    prediction_text = ""
    frame_count = 0
    last_prediction = None
    
    # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
    try:
        with open('test.txt', 'w') as f:
            f.write("æ‰‹å†™æ•°å­—è¯†åˆ«ç»“æœ:\n")
        print("âœ… test.txtæ–‡ä»¶åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ test.txtæ–‡ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")

    while cap.isOpened():
        success, img = cap.read()
        if not success:
            print("é”™è¯¯: æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
            break

        img = cv2.flip(img, 1)
        img = detector.findHands(img)
        lmList = detector.findPosition(img, draw=False)
        a1,b1,c1,d1=400,0,400,400
        cv2.rectangle(img,(a1,b1),(a1+c1,b1+d1),(0,255,0),0)
        if len(lmList) != 0:
            x1, y1 = lmList[8][1:]
            x2, y2 = lmList[12][1:]

            fingers = detector.fingersUp()

            if fingers[1] and fingers[2] == False:
                cv2.circle(img, (x1,y1), 15, drawColor, cv2.FILLED)
                if xp == 0 and yp == 0:
                    xp, yp = x1, y1
                cv2.line(img, (xp, yp), (x1, y1), drawColor, brushThickness)
                cv2.line(imgCanvas, (xp, yp), (x1, y1), drawColor, brushThickness)
            xp, yp = x1, y1

        frame_count += 1
        if frame_count % 10 == 0:
            cv2.imwrite("./test.jpg", imgCanvas)
            roi_debug = imgCanvas[0:400, 400:800]
            roi_sum = np.sum(roi_debug)
            if roi_sum > 0:
                predicted_digit = recognize_digit_from_image("./test.jpg")
                if predicted_digit is not None:
                    print(f"[{predicted_digit}]")
                    prediction_text = f"{predicted_digit}"
                    if ui is not None:
                        ui.textBrowser.append(str(predicted_digit))
                    try:
                        with open('test.txt', 'a') as f:
                            f.write(f"[{predicted_digit}]\n")
                    except Exception as e:
                        print(f"æ–‡ä»¶å†™å…¥é”™è¯¯: {e}")
                    if last_prediction != predicted_digit:
                        print(f"ğŸ”¥ è¯†åˆ«åˆ°æ–°æ•°å­—: {predicted_digit}")
                        last_prediction = predicted_digit
                else:
                    prediction_text = "è¯†åˆ«å¤±è´¥"
                    print("âš ï¸  ç”»å¸ƒæœ‰å†…å®¹ä½†è¯†åˆ«å¤±è´¥")
            else:
                prediction_text = ""
                if frame_count % 100 == 0:
                    print("â„¹ï¸  ç”»å¸ƒåŒºåŸŸä¸ºç©º")

        img = cv2.bitwise_or(img, imgCanvas)
        if prediction_text and prediction_text != "è¯†åˆ«å¤±è´¥":
            cv2.putText(img, ' %s' % prediction_text, (90, 90), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 0), 5, cv2.LINE_AA)
        elif prediction_text == "è¯†åˆ«å¤±è´¥":
            cv2.putText(img, "è¯†åˆ«å¤±è´¥", (90, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)

        cv2.imshow("Canvas", imgCanvas)
        cv2.imshow("Image", img)
        cv2.imwrite("./test.jpg", imgCanvas)
        
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('c') or key == ord('C'):
            imgCanvas = np.zeros((720, 1280, 3), np.uint8)
            prediction_text = ""
            last_prediction = None
            print("ğŸ§¹ ç”»å¸ƒå·²æ¸…ç©ºï¼Œå‡†å¤‡è¯†åˆ«æ–°æ•°å­—")
            cv2.imwrite("./test.jpg", imgCanvas)
            try:
                with open('test.txt', 'a') as f:
                    f.write("--- ç”»å¸ƒå·²æ¸…ç©º ---\n")
            except:
                pass
    cap.release()
    cv2.destroyAllWindows()
